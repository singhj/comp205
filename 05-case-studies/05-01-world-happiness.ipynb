{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img align=\"right\" style=\"padding-left:10px; height: 60%; width: 20%\" src=\"figures/whr.png\" >\n",
    "\n",
    "# World Happiness Report\n",
    "\n",
    "This case study is based on the 7th World Happiness Report. The first was released in April 2012 in support of a UN High level meeting on “Wellbeing and Happiness: Defining a New Economic Paradigm”. \n",
    "\n",
    "That 2012 report presented the available global data on national happiness and reviewed related evidence from the emerging science of happiness, showing that the quality of people’s lives can be coherently, reliably, and validly assessed by a variety of subjective well-being measures, collectively referred to then and in subsequent reports as “happiness.” \n",
    "\n",
    "This year’s World Happiness Report focuses on happiness and the community: how happiness has evolved over the past dozen years, with a focus on the technologies, social norms, conflicts and government policies that have driven those changes.\n",
    "\n",
    "I have downloaded the data from [Chapter 2: Online Data](https://s3.amazonaws.com/happiness-report/2019/Chapter2OnlineData.xls) and filtered out data prior to 2018. The result is available in CSV format, as the next cell shows. _Data Prep Notes:_ The Happiness Score column is from Figure 2.6 in the downloaded report; the other data columns are from Table 2.1 in the same report. If a country wasn't in either list, it wasn't included in the CSV file.\n",
    "\n",
    "We were first introduced to this dataset in `03-04-world-happiness` and `03-05-world-happiness.ipynb`. We return to the dataset for a deeper analysis!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Country</th>\n",
       "      <th>Year</th>\n",
       "      <th>HappinessScore</th>\n",
       "      <th>LifeLadder</th>\n",
       "      <th>LogGDP</th>\n",
       "      <th>SocialSupport</th>\n",
       "      <th>HealthyLifeExpectancyAtBirth</th>\n",
       "      <th>FreedomToMakeLifeChoices</th>\n",
       "      <th>Generosity</th>\n",
       "      <th>PerceptionsOfCorruption</th>\n",
       "      <th>PositiveAffect</th>\n",
       "      <th>NegativeAffect</th>\n",
       "      <th>ConfidenceInNationalGovernment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>Afghanistan</td>\n",
       "      <td>2018</td>\n",
       "      <td>3.203</td>\n",
       "      <td>2.694303</td>\n",
       "      <td>7.494588</td>\n",
       "      <td>0.507516</td>\n",
       "      <td>52.599998</td>\n",
       "      <td>0.373536</td>\n",
       "      <td>-0.084888</td>\n",
       "      <td>0.927606</td>\n",
       "      <td>0.424125</td>\n",
       "      <td>0.404904</td>\n",
       "      <td>0.364666</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>Albania</td>\n",
       "      <td>2018</td>\n",
       "      <td>4.719</td>\n",
       "      <td>5.004403</td>\n",
       "      <td>9.412399</td>\n",
       "      <td>0.683592</td>\n",
       "      <td>68.699997</td>\n",
       "      <td>0.824212</td>\n",
       "      <td>0.005385</td>\n",
       "      <td>0.899129</td>\n",
       "      <td>0.713300</td>\n",
       "      <td>0.318997</td>\n",
       "      <td>0.435338</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>Algeria</td>\n",
       "      <td>2018</td>\n",
       "      <td>5.211</td>\n",
       "      <td>5.043086</td>\n",
       "      <td>9.557952</td>\n",
       "      <td>0.798651</td>\n",
       "      <td>65.900002</td>\n",
       "      <td>0.583381</td>\n",
       "      <td>-0.172413</td>\n",
       "      <td>0.758704</td>\n",
       "      <td>0.591043</td>\n",
       "      <td>0.292946</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>Argentina</td>\n",
       "      <td>2018</td>\n",
       "      <td>6.086</td>\n",
       "      <td>5.792797</td>\n",
       "      <td>9.809972</td>\n",
       "      <td>0.899912</td>\n",
       "      <td>68.800003</td>\n",
       "      <td>0.845895</td>\n",
       "      <td>-0.206937</td>\n",
       "      <td>0.855255</td>\n",
       "      <td>0.820310</td>\n",
       "      <td>0.320502</td>\n",
       "      <td>0.261352</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>Armenia</td>\n",
       "      <td>2018</td>\n",
       "      <td>4.559</td>\n",
       "      <td>5.062449</td>\n",
       "      <td>9.119424</td>\n",
       "      <td>0.814449</td>\n",
       "      <td>66.900002</td>\n",
       "      <td>0.807644</td>\n",
       "      <td>-0.149109</td>\n",
       "      <td>0.676826</td>\n",
       "      <td>0.581488</td>\n",
       "      <td>0.454840</td>\n",
       "      <td>0.670828</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>131</td>\n",
       "      <td>Venezuela</td>\n",
       "      <td>2018</td>\n",
       "      <td>4.707</td>\n",
       "      <td>5.005663</td>\n",
       "      <td>9.270281</td>\n",
       "      <td>0.886882</td>\n",
       "      <td>66.500000</td>\n",
       "      <td>0.610855</td>\n",
       "      <td>-0.176156</td>\n",
       "      <td>0.827560</td>\n",
       "      <td>0.759221</td>\n",
       "      <td>0.373658</td>\n",
       "      <td>0.260700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>132</td>\n",
       "      <td>Vietnam</td>\n",
       "      <td>2018</td>\n",
       "      <td>5.175</td>\n",
       "      <td>5.295547</td>\n",
       "      <td>8.783416</td>\n",
       "      <td>0.831945</td>\n",
       "      <td>67.900002</td>\n",
       "      <td>0.909260</td>\n",
       "      <td>-0.039124</td>\n",
       "      <td>0.808423</td>\n",
       "      <td>0.692222</td>\n",
       "      <td>0.191061</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>133</td>\n",
       "      <td>Yemen</td>\n",
       "      <td>2018</td>\n",
       "      <td>3.380</td>\n",
       "      <td>3.057514</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.789422</td>\n",
       "      <td>56.700001</td>\n",
       "      <td>0.552726</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.792587</td>\n",
       "      <td>0.461114</td>\n",
       "      <td>0.314870</td>\n",
       "      <td>0.308151</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>134</td>\n",
       "      <td>Zambia</td>\n",
       "      <td>2018</td>\n",
       "      <td>4.107</td>\n",
       "      <td>4.041488</td>\n",
       "      <td>8.223958</td>\n",
       "      <td>0.717720</td>\n",
       "      <td>55.299999</td>\n",
       "      <td>0.790626</td>\n",
       "      <td>0.036644</td>\n",
       "      <td>0.810731</td>\n",
       "      <td>0.702698</td>\n",
       "      <td>0.350963</td>\n",
       "      <td>0.606715</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>135</td>\n",
       "      <td>Zimbabwe</td>\n",
       "      <td>2018</td>\n",
       "      <td>3.663</td>\n",
       "      <td>3.616480</td>\n",
       "      <td>7.553395</td>\n",
       "      <td>0.775388</td>\n",
       "      <td>55.599998</td>\n",
       "      <td>0.762675</td>\n",
       "      <td>-0.038384</td>\n",
       "      <td>0.844209</td>\n",
       "      <td>0.710119</td>\n",
       "      <td>0.211726</td>\n",
       "      <td>0.550508</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>136 rows × 13 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         Country  Year  HappinessScore  LifeLadder    LogGDP  SocialSupport  \\\n",
       "0    Afghanistan  2018           3.203    2.694303  7.494588       0.507516   \n",
       "1        Albania  2018           4.719    5.004403  9.412399       0.683592   \n",
       "2        Algeria  2018           5.211    5.043086  9.557952       0.798651   \n",
       "3      Argentina  2018           6.086    5.792797  9.809972       0.899912   \n",
       "4        Armenia  2018           4.559    5.062449  9.119424       0.814449   \n",
       "..           ...   ...             ...         ...       ...            ...   \n",
       "131    Venezuela  2018           4.707    5.005663  9.270281       0.886882   \n",
       "132      Vietnam  2018           5.175    5.295547  8.783416       0.831945   \n",
       "133        Yemen  2018           3.380    3.057514       NaN       0.789422   \n",
       "134       Zambia  2018           4.107    4.041488  8.223958       0.717720   \n",
       "135     Zimbabwe  2018           3.663    3.616480  7.553395       0.775388   \n",
       "\n",
       "     HealthyLifeExpectancyAtBirth  FreedomToMakeLifeChoices  Generosity  \\\n",
       "0                       52.599998                  0.373536   -0.084888   \n",
       "1                       68.699997                  0.824212    0.005385   \n",
       "2                       65.900002                  0.583381   -0.172413   \n",
       "3                       68.800003                  0.845895   -0.206937   \n",
       "4                       66.900002                  0.807644   -0.149109   \n",
       "..                            ...                       ...         ...   \n",
       "131                     66.500000                  0.610855   -0.176156   \n",
       "132                     67.900002                  0.909260   -0.039124   \n",
       "133                     56.700001                  0.552726         NaN   \n",
       "134                     55.299999                  0.790626    0.036644   \n",
       "135                     55.599998                  0.762675   -0.038384   \n",
       "\n",
       "     PerceptionsOfCorruption  PositiveAffect  NegativeAffect  \\\n",
       "0                   0.927606        0.424125        0.404904   \n",
       "1                   0.899129        0.713300        0.318997   \n",
       "2                   0.758704        0.591043        0.292946   \n",
       "3                   0.855255        0.820310        0.320502   \n",
       "4                   0.676826        0.581488        0.454840   \n",
       "..                       ...             ...             ...   \n",
       "131                 0.827560        0.759221        0.373658   \n",
       "132                 0.808423        0.692222        0.191061   \n",
       "133                 0.792587        0.461114        0.314870   \n",
       "134                 0.810731        0.702698        0.350963   \n",
       "135                 0.844209        0.710119        0.211726   \n",
       "\n",
       "     ConfidenceInNationalGovernment  \n",
       "0                          0.364666  \n",
       "1                          0.435338  \n",
       "2                               NaN  \n",
       "3                          0.261352  \n",
       "4                          0.670828  \n",
       "..                              ...  \n",
       "131                        0.260700  \n",
       "132                             NaN  \n",
       "133                        0.308151  \n",
       "134                        0.606715  \n",
       "135                        0.550508  \n",
       "\n",
       "[136 rows x 13 columns]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "data1 = pd.read_csv('happiness-report.csv')\n",
    "data1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Some observations:\n",
    "\n",
    "* **Quality** Some data points just show `NaN` (not a number where there should be one). This is typically done using `dropna()`. \n",
    "\n",
    "* **Normalization:** The columns have different ranges: some values are between 0. and 1., Others have different ranges. For example Generosity is between -0.33 and 0.49, and so on. Un-normalized ranges can cause the different features to be under- or over-valued during analysis. The data needs to be preprocessed so as to be uniform. \n",
    "\n",
    "* **Pre-normalization:** `LogGDP` is the logarithm of the GDP per capita. Here is a case where the data has gone through Feature Scaling _prior to publication!_ Taking a log of numbers whose range spans multiple orders of magnitude is a common technique for compressing the range. However, it still doesn't span the range [0. 1.] and a bit more Feature Scaling will be required."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Quality\n",
    "\n",
    "First, let's try `dropna()`. Out of 136 rows, _we are left with 113_. \n",
    "\n",
    "How important is it for data quality to be _absolutely_ sacrosanct in our analysis? To answer this question, we wish to compare `data1` with `data1.dropna()`. Would we lose any important data by doing so? Create a DataFrame `data_na` that shows what we would lose by using `dropna()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quality Evaluation\n",
    "\n",
    "Many of the countries in the \"to be dropped\" list are important in a geopolitical sense. For an analysis of a United Nations dataset called **World Happiness Report**, there are three options:\n",
    "\n",
    "1. Hold the line on data quality and publish with just 113 countries in the final report,\n",
    "1. Fill the missing column values with the average value for that column, or\n",
    "1. (Partial.) Drop the rows with `NaN` for \"objective\" features such as `LogGDP` and fill the unavailable features with the average values of those features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1a. Choosing the Quality Option\n",
    "\n",
    "Which of the above quality options would you choose and why? \n",
    "\n",
    "_This is a judgment question. No answer is \"wrong\" or \"preferred.\" How well you make your argument is what's important!_\n",
    "\n",
    "---\n",
    "\n",
    "---\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1b. Filled Data\n",
    "\n",
    "Irrespective of your answer to Q 1a, management has decided to pursue option 3. \n",
    "\n",
    "Prepare a DataFrame which has only non-NaN LogGDP rows and the remaining feature values filled with average values of those features. Hint: check out the `np.nanmean()` function.\n",
    "\n",
    "Finally, reindex the cleaned up DataFrame and rename it `data1_rdy`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Normalizing the data\n",
    "\n",
    "The process of making all columns uniform in scale is referred to as **Feature Scaling**. Many data analysis libraries require it (see [_The Importance of Feature Scaling_](https://scikit-learn.org/stable/auto_examples/preprocessing/plot_scaling_importance.html)). Scikit-Learn offers an extensive library for [preprocessing data](https://scikit-learn.org/stable/modules/classes.html#module-sklearn.preprocessing). Pay particular attention to `*Scaler()` functions. Also Discretizers such as Binarizer, KBinsDiscretizer and QuantileTransformer.\n",
    "\n",
    "Use `StandardScaler` to scale all features except `Country`, `Year` and `LogGDP`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler, RobustScaler\n",
    "scaler = StandardScaler()\n",
    "# Fill in..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Examine scaled features\n",
    "\n",
    "Examine min, mean and max values of each of the scaled features. Why are the min and max values not 0 and 1 respectively?\n",
    "\n",
    "**Your Answer**\n",
    "\n",
    "---\n",
    "\n",
    "---\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Cluster the data into 2 clusters.\n",
    "\n",
    "The clustering will return a new column with values 0 or 1. We won't yet know what 0 and 1 stand for. Show `data_scaled` along with the returned class values. `kmeans.labels_` gives the class values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "kmeans = # fill in"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assigning Cluster Labels\n",
    "\n",
    "We need to assign labels for values of the clustering cells. The Scikit Learn class LabelEncoder is designed for this purpose. Its usage is shown in the next cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.utils import column_or_1d\n",
    "\n",
    "le = LabelEncoder()\n",
    "le.fit(['unhappy', 'happy'])\n",
    "\n",
    "def clustered_pd(np_data, np_rows, np_cols, labels, ole):\n",
    "    return pd.concat([\n",
    "        pd.DataFrame(np_rows, columns=['Country']), \n",
    "        pd.DataFrame(ole.inverse_transform(labels), columns=['Cluster']), \n",
    "        pd.DataFrame(np_data, dtype='float32', columns=np_cols)], axis=1)\n",
    "\n",
    "print (clustered_pd(np_norm, np_rows, np_cols, kmeans.labels_, le).loc[:,['Country', 'Cluster']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Assigning Cluster Labels\n",
    "\n",
    "The labels produced above are **wrong**. Happiness values of 0 are being translated as 'happy'. Fix the code in the above cell to correct this error."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Make 5 clusters\n",
    "\n",
    "Based on the above exercise, cluster `data1_scaled` into 5 classes, giving values 1 &hellip; 5 to the appropriate rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "n_clusters = 5\n",
    "kmeans = KMeans(n_clusters=n_clusters, random_state=0).fit(np_norm)\n",
    "kmeans.labels_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Automatically assigning labels\n",
    "\n",
    "Replace arbitrary numeric labels with descriptive numerical labels for categories. Sometimes it is possible to use some column values to give us a hint about cluster identity. For example, in the data above, Happiness Score could be used to label the rows.\n",
    "\n",
    "Change the happiness values to be 'sad', 'down', 'ok', 'up', 'happy'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# When you're done, submit the notebook\n",
    "\n",
    "1. **Run all the cells in order.**\n",
    "\n",
    "2. Submit the notebook by saving it as PDF. \n",
    "    * In the cluster environment, it's File | Print (Save as PDF) and submit to [Gradescope](https://www.gradescope.com/courses/182658)<sup>&dagger;</sup>, \n",
    "    * On other versions, it may be File | Download As (PDF) and then submit to [Gradescope](https://www.gradescope.com/courses/182658)<sup>&dagger;</sup>.\n",
    "\n",
    "<sup>&dagger;</sup>To submit to Gradescope, log into the website, add course 9W7PW3 (if not already added) and submit. The assignment name should match the name of this notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![The end](https://live.staticflickr.com/32/89187454_3ae6aded89_b.jpg)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
